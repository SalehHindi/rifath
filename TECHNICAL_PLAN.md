# LiveKit Voice Agent with Visual UI - Technical Plan

## Overview

This project implements a React/Next.js web application with a LiveKit voice agent that can control UI components through voice commands. The agent runs locally as a Python process and communicates with the frontend via LiveKit's RPC (Remote Procedure Calls) mechanism.

---

## Current Status (Dec 11, 2025)

- âœ… **LiveKit + Push-to-Talk pipeline working**: Local LiveKit server, token endpoint, room connection, push-to-talk button, and agent audio playback hooks are all in place.
- âœ… **Agent session lifecycle fixed**: The Python worker stays running and only listens when the user holds Push-to-Talk.
- âœ… **Mode renderer scaffolding**: Placeholder Quiz/Table/Blank components render correctly based on `ModeContext`.
- âœ… **RPC infrastructure**: `set_mode`/`get_mode` RPC handlers and `ModeContext` wiring are implemented on the frontend.
- ðŸŸ¡ **Agent tool definitions created**: `show_quiz`, `show_table`, `show_blank`, `get_current_mode` exist, but we still need to polish the prompts and verify tool-triggered UI changes end-to-end.
- ðŸŸ¡ **Frontend audio playback**: `useAgentAudio` attaches the agentâ€™s audio track; testing shows the agent speaks, though weâ€™re monitoring for edge cases.
- â³ **Quiz experience**: Quiz data, animations, tool-driven quiz control, and RPC sync are scheduled for Phase 3.
- Next immediate task: **finish Phase 2 by verifying the tool calls change the rendered component (quiz/table/blank) reliably** before moving into Phase 3â€™s quiz UX work.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (Next.js)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Push-to-Talk â”‚  â”‚  Mode Render â”‚  â”‚   Quiz UI    â”‚     â”‚
â”‚  â”‚    Button    â”‚  â”‚    Area      â”‚  â”‚  Component   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚           â”‚                â”‚                  â”‚             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                            â”‚                                â”‚
â”‚                    LiveKit Client SDK                        â”‚
â”‚                            â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    WebRTC Connection
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LiveKit Server                           â”‚
â”‚              (Local or Cloud Instance)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    WebRTC Connection
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Python Agent (Local Process)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   STT/LLM    â”‚  â”‚  Tool Calls  â”‚  â”‚  RPC Methods  â”‚     â”‚
â”‚  â”‚   Pipeline   â”‚  â”‚   Handler    â”‚  â”‚   Registry    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Technology Stack

### Frontend
- **Framework**: Next.js 14+ (App Router)
- **UI Library**: React 18+
- **Styling**: Tailwind CSS
- **LiveKit SDK**: `@livekit/components-react`, `livekit-client`
- **State Management**: React Context API / Zustand (for mode and quiz state)

### Backend Agent
- **Language**: Python 3.10+
- **Framework**: LiveKit Agents SDK (`livekit-agents`)
- **LLM**: OpenAI GPT-4 or compatible (can use local models)
- **STT**: OpenAI Whisper or LiveKit's built-in STT
- **TTS**: OpenAI TTS or LiveKit's built-in TTS

### Infrastructure
- **LiveKit Server**: Local instance via Docker or LiveKit Cloud (free tier)
- **Token Server**: Next.js API route for token generation

## Repository Structure

```
rifath_project/
â”œâ”€â”€ frontend/                    # Next.js application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ page.tsx            # Main page with push-to-talk
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ token/
â”‚   â”‚           â””â”€â”€ route.ts    # Token generation endpoint
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ VoiceAgent/
â”‚   â”‚   â”‚   â”œâ”€â”€ PushToTalkButton.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioVisualizer.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ConnectionStatus.tsx
â”‚   â”‚   â”œâ”€â”€ Quiz/
â”‚   â”‚   â”‚   â”œâ”€â”€ QuizComponent.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ QuestionCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ OptionButton.tsx
â”‚   â”‚   â”‚   â””â”€â”€ NextButton.tsx
â”‚   â”‚   â”œâ”€â”€ Table/
â”‚   â”‚   â”‚   â””â”€â”€ TableComponent.tsx
â”‚   â”‚   â””â”€â”€ Blank/
â”‚   â”‚       â””â”€â”€ BlankComponent.tsx
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useLiveKitRoom.ts
â”‚   â”‚   â”œâ”€â”€ useVoiceAgent.ts
â”‚   â”‚   â””â”€â”€ useQuiz.ts
â”‚   â”œâ”€â”€ contexts/
â”‚   â”‚   â”œâ”€â”€ ModeContext.tsx     # Manages current UI mode
â”‚   â”‚   â””â”€â”€ QuizContext.tsx     # Manages quiz state
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ livekit.ts          # LiveKit client setup
â”‚   â”‚   â””â”€â”€ quiz-data.ts        # Hardcoded quiz data
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ agent/                       # Python agent
â”‚   â”œâ”€â”€ main.py                 # Agent entry point
â”‚   â”œâ”€â”€ agent.py                # Main agent logic
â”‚   â”œâ”€â”€ tools.py                # Tool definitions for UI control
â”‚   â”œâ”€â”€ quiz_handler.py         # Quiz-specific logic
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ docker-compose.yml          # Local LiveKit server (optional)
â”œâ”€â”€ README.md
â”œâ”€â”€ TECHNICAL_PLAN.md
â””â”€â”€ .gitignore
```

## Phase Breakdown

---

## Phase 1: Base UI and Push-to-Talk

### Objectives
- Set up Next.js project with LiveKit integration
- Implement push-to-talk button functionality
- Create basic component rendering area
- Establish connection between frontend and agent

### Implementation Steps

#### 1.1 Project Setup
- [x] Initialize Next.js project with TypeScript
- [x] Install LiveKit dependencies:
  - `livekit-client`
  - `@livekit/components-react`
  - `@livekit/components-styles`
- [x] Set up Tailwind CSS
- [x] Create basic project structure

#### 1.2 LiveKit Configuration
- [x] Set up LiveKit server (local Docker or Cloud)
- [x] Create token generation API route (`/api/token/route.ts`)
- [x] Configure environment variables:
  - `LIVEKIT_URL`
  - `LIVEKIT_API_KEY`
  - `LIVEKIT_API_SECRET`

#### 1.3 Push-to-Talk Button
- [x] Create `PushToTalkButton` component
- [x] Implement mouse/touch event handlers:
  - `onMouseDown` / `onTouchStart`: Start listening
  - `onMouseUp` / `onTouchEnd`: Stop listening
  - `onMouseLeave`: Stop listening (if button released outside)
- [x] Visual feedback:
  - Active state styling (e.g., red background when listening)
  - Disabled state when not connected
- [x] Integrate with LiveKit room to control microphone track publishing

#### 1.4 Basic Agent Setup
- [x] Initialize Python agent project
- [x] Set up basic agent that connects to room
- [x] Configure STT/LLM/TTS pipeline (OpenAI Whisper/LLM/TTS + silero VAD)
- [x] Test basic voice interaction

#### 1.5 Component Rendering Area
- [x] Create main layout with:
  - Push-to-talk button at top
  - Content area below for dynamic components
- [x] Create placeholder component to show in Phase 1
- [x] Set up basic state management for component rendering

### Technical Details

**Push-to-Talk Implementation:**
```typescript
// Pseudocode
const handleMouseDown = () => {
  if (room && room.localParticipant) {
    // Enable microphone track
    room.localParticipant.setMicrophoneEnabled(true);
    setIsListening(true);
  }
};

const handleMouseUp = () => {
  if (room && room.localParticipant) {
    // Disable microphone track
    room.localParticipant.setMicrophoneEnabled(false);
    setIsListening(false);
  }
};
```

**LiveKit Room Connection:**
- Use `useRoom` hook from `@livekit/components-react`
- Connect to room on component mount
- Handle connection state changes
- Subscribe to agent participant's audio track

### Deliverables
- âœ… Working push-to-talk button
- âœ… Basic voice agent connection
- âœ… Component rendering area displaying placeholder component
- âœ… Visual feedback for connection and listening states

---

## Phase 2: Mode Switching via Tool Calls

### Objectives
- Implement RPC methods for UI mode control
- Create tool definitions in agent for mode switching
- Build different UI components (Quiz, Table, Blank)
- Enable voice commands to switch between modes

### Implementation Steps

#### 2.1 RPC Method Registration (Frontend)
- [x] Register RPC methods in frontend:
  - `set_mode` - Changes the current UI mode
  - `get_mode` - Returns current mode (for agent awareness)
- [x] Create `ModeContext` to manage current mode state
- [x] Implement mode switching logic

#### 2.2 Tool Definitions (Agent)
- [x] Define tools in agent:
  - `show_quiz` - Triggers quiz mode
  - `show_table` - Triggers table mode
  - `show_blank` - Triggers blank mode
- [x] Map tool calls to RPC method invocations
- [x] Add tool descriptions for LLM understanding

#### 2.3 UI Components
- [x] **Quiz Component** (placeholder):
  - Basic structure with title
  - Placeholder for questions/options
- [x] **Table Component**:
  - Hardcoded table data
  - Simple table rendering
- [x] **Blank Component**:
  - Empty component or minimal placeholder

#### 2.4 Mode Rendering Logic
- [x] Create mode renderer component that switches based on state
- [ ] Implement smooth transitions between modes (optional polish)
- [x] Handle mode state persistence during session

#### 2.5 Agent Tool Call Integration
- [ ] Configure agent to recognize mode-switching commands (LLM/tool prompt tuning in progress)
- [ ] Test voice commands:
  - "Show a quiz"
  - "Show a table"
  - "Show nothing" / "Clear screen"
- [ ] Add error handling for invalid modes

### Technical Details

**RPC Method Registration:**
```typescript
// Frontend - Register RPC method
room.registerRpcMethod('set_mode', async (request) => {
  const { mode } = request.payload as { mode: string };
  setCurrentMode(mode);
  return { success: true };
});
```

**Agent Tool Definition:**
```python
# Agent - Define tool
@agent.tool()
async def show_quiz() -> str:
    """Show a quiz interface to the user."""
    result = await room.rpc.call('set_mode', {'mode': 'quiz'})
    return "Quiz interface is now displayed."
```

**Mode Types:**
```typescript
type UIMode = 'blank' | 'quiz' | 'table';
```

### Deliverables
- ðŸŸ¡ Voice commands can switch UI modes (tools defined; need final verification)
- âœ… Three distinct UI components (Quiz, Table, Blank)
- ðŸŸ¡ Smooth transitions between modes (basic switch works; animation polish pending)
- âœ… Agent can query current mode

---

## Phase 3: Quiz Functionality

### Objectives
- Implement full quiz component with questions and options
- Add quiz data loading and state management
- Create option selection with animations
- Integrate quiz state with agent via RPC
- Implement next question navigation

### Implementation Steps

#### 3.1 Quiz Data Structure
- [ ] Create quiz data file with multiple questions:
  ```typescript
  interface QuizQuestion {
    id: string;
    question: string;
    options: { [key: string]: string }; // { A: "Option 1", B: "Option 2", ... }
    correctAnswer: string; // "A", "B", "C", or "D"
  }
  ```
- [ ] Hardcode quiz data (simulating backend response)
- [ ] Create quiz data loader utility

#### 3.2 Quiz State Management
- [ ] Create `QuizContext` for global quiz state:
  - Current question index
  - Selected option
  - Quiz questions array
  - Quiz status (idle, active, completed)
- [ ] Implement quiz initialization
- [ ] Handle question progression

#### 3.3 Quiz Component Implementation
- [ ] **QuestionCard Component**:
  - Display current question text
  - Show question number (e.g., "Question 1 of 5")
- [ ] **OptionButton Component**:
  - Render each option (A, B, C, D)
  - Handle click events
  - Visual states: default, selected, correct, incorrect
  - Cute animation on selection (e.g., scale, color transition)
- [ ] **NextButton Component**:
  - Show only when answer is selected
  - Navigate to next question
  - Handle quiz completion

#### 3.4 RPC Methods for Quiz Control
- [ ] Register RPC methods:
  - `load_quiz` - Loads quiz data
  - `select_option` - Selects an option (can be called by agent)
  - `next_question` - Moves to next question
  - `get_quiz_state` - Returns current quiz state
- [ ] Update agent with quiz state in context

#### 3.5 Agent Integration
- [ ] Add tool: `request_quiz` - Triggers quiz loading
- [ ] Add tool: `select_quiz_option` - Allows agent to select option via voice
- [ ] Update agent prompt to include:
  - Current question text
  - Available options
  - Current quiz state
- [ ] Handle voice commands:
  - "Give me a trivia question" â†’ Load quiz
  - "Select A" / "Choose B" â†’ Select option
  - "Next question" â†’ Move to next

#### 3.6 Animations and Visual Feedback
- [ ] Implement selection animation:
  - Scale effect on click
  - Color transition (e.g., blue â†’ green/red)
  - Smooth transitions
- [ ] Show correct/incorrect feedback:
  - Green highlight for correct
  - Red highlight for incorrect
  - Visual indicator (checkmark/X)
- [ ] Add loading states

#### 3.7 Quiz State Synchronization
- [ ] Send quiz state updates to agent via data channel or RPC
- [ ] Agent includes quiz context in LLM prompts
- [ ] Handle quiz completion state

### Technical Details

**Quiz Data Example:**
```typescript
const quizData: QuizQuestion[] = [
  {
    id: '1',
    question: 'What is the capital of France?',
    options: {
      A: 'London',
      B: 'Berlin',
      C: 'Paris',
      D: 'Madrid'
    },
    correctAnswer: 'C'
  },
  // ... more questions
];
```

**Option Selection Animation:**
```typescript
// Use framer-motion or CSS transitions
const OptionButton = ({ option, isSelected, isCorrect, onClick }) => {
  return (
    <motion.button
      whileHover={{ scale: 1.05 }}
      whileTap={{ scale: 0.95 }}
      animate={{
        backgroundColor: isSelected 
          ? (isCorrect ? 'green' : 'red')
          : 'gray'
      }}
      onClick={onClick}
    >
      {option}
    </motion.button>
  );
};
```

**Agent Tool for Quiz:**
```python
@agent.tool()
async def request_quiz() -> str:
    """Load and display a quiz to the user."""
    result = await room.rpc.call('load_quiz', {})
    quiz_state = await room.rpc.call('get_quiz_state', {})
    
    # Include quiz state in agent context
    question = quiz_state['currentQuestion']
    options = quiz_state['options']
    
    return f"Quiz loaded. Current question: {question}. Options: {options}"
```

**Voice Command Handling:**
- Agent listens for: "select A", "choose B", "answer C", etc.
- Agent calls `select_quiz_option` RPC with selected option
- Frontend updates UI and shows feedback

### Deliverables
- âœ… Full quiz component with multiple questions
- âœ… Option selection with animations
- âœ… Correct/incorrect visual feedback
- âœ… Next question navigation
- âœ… Voice commands for quiz interaction
- âœ… Quiz state synchronized with agent

---

## Integration Points

### Frontend â†” Agent Communication

1. **RPC Methods (Frontend â†’ Agent)**:
   - Agent calls frontend methods to control UI
   - Methods: `set_mode`, `load_quiz`, `select_option`, `next_question`

2. **RPC Methods (Agent â†’ Frontend)**:
   - Frontend can query agent state (optional)
   - Method: `get_agent_state`

3. **Data Channel** (Optional):
   - Send quiz state updates to agent
   - Receive agent context updates

4. **Participant Attributes**:
   - Agent state (`lk.agent.state`) for UI indicators
   - Custom attributes for mode/quiz state

### State Flow

```
User Voice Command
    â†“
Agent STT â†’ LLM â†’ Tool Call
    â†“
RPC Call to Frontend
    â†“
Frontend State Update
    â†“
UI Re-render
    â†“
Visual Feedback
```

## Environment Variables

### Frontend (.env.local)
```bash
NEXT_PUBLIC_LIVEKIT_URL=ws://localhost:7880
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_api_secret
```

### Agent (.env)
```bash
LIVEKIT_URL=ws://localhost:7880
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_api_secret
OPENAI_API_KEY=your_openai_key  # If using OpenAI
```

## Testing Strategy

### Phase 1 Testing
- [ ] Test push-to-talk button functionality
- [ ] Verify LiveKit connection
- [ ] Test microphone enable/disable
- [ ] Verify agent audio playback

### Phase 2 Testing
- [ ] Test each mode switch command
- [ ] Verify RPC method calls
- [ ] Test mode persistence
- [ ] Verify error handling

### Phase 3 Testing
- [ ] Test quiz loading
- [ ] Test option selection (click and voice)
- [ ] Verify animations
- [ ] Test question progression
- [ ] Test quiz completion
- [ ] Verify state synchronization

## Deployment Considerations

### Local Development
- Run LiveKit server via Docker: `docker run -p 7880:7880 livekit/livekit-server --dev`
- Or use LiveKit Cloud free tier
- Run Next.js dev server: `npm run dev`
- Run Python agent: `python agent/main.py`

### Production (Future)
- Deploy LiveKit server or use LiveKit Cloud
- Deploy Next.js to Vercel/Netlify
- Deploy Python agent to cloud service (Railway, Render, etc.)
- Set up proper authentication and token generation

## Security Considerations

1. **Token Generation**: Never expose API keys in frontend code
2. **RPC Validation**: Validate all RPC method inputs
3. **Rate Limiting**: Implement rate limiting on token endpoint
4. **CORS**: Configure CORS properly for LiveKit server

## Future Enhancements

- Real backend API for quiz data
- Multiple quiz categories
- Score tracking
- Quiz history
- More interactive components (charts, forms, etc.)
- Multi-user support
- Quiz sharing capabilities

## Dependencies

### Frontend
```json
{
  "dependencies": {
    "next": "^14.0.0",
    "react": "^18.0.0",
    "react-dom": "^18.0.0",
    "livekit-client": "^2.0.0",
    "@livekit/components-react": "^2.0.0",
    "@livekit/components-styles": "^2.0.0",
    "framer-motion": "^10.0.0",
    "zustand": "^4.0.0"
  }
}
```

### Agent
```txt
livekit-agents>=0.10.0
livekit-api>=0.10.0
openai>=1.0.0
python-dotenv>=1.0.0
```

## Timeline Estimate

- **Phase 1**: 4-6 hours
- **Phase 2**: 3-4 hours
- **Phase 3**: 6-8 hours
- **Total**: ~13-18 hours

## Notes

- Use LiveKit's free tier or local server for development
- Agent runs as local process, not deployed service
- All communication happens via WebRTC through LiveKit server
- RPC methods provide bidirectional communication
- Quiz data is hardcoded but structured to be easily replaced with API calls

